{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongpochen/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from Code.Utils.Read import read_config\n",
    "from Code.Utils.Logging import setup_logging\n",
    "from Code.Model.Model import Transformer\n",
    "from Code.Train.Train import train_epoch\n",
    "from Code.SFT.DataClean.CleanAlpacaGpt4 import cleanAlpacaGpt4File\n",
    "from Code.DataSet.Dataset import PretrainDataset\n",
    "from Code.Tokenizer.Tokenizer import DataPreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "\n",
    "print(input.size())\n",
    "print(target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"embed_dim\" : 512,\n",
    "    \"max_seq_len\" : 512,\n",
    "    \"n_layers\" : 2,\n",
    "    \"n_heads\" : 2,\n",
    "    \"multiple_of\" : 32,\n",
    "    \"dropout\" : 0.0,\n",
    "    \"bias\" : False,\n",
    "    \"learning_rate\" : 3e-4 ,\n",
    "    \"weight_decay\" : 1e-1,\n",
    "    \"beta1\" : 0.9,\n",
    "    \"beta2\" : 0.95,\n",
    "    \"grad_clip\" : 1.0,\n",
    "    \"batch_size\" : 32,\n",
    "    \"vocab_size\" : 64793,\n",
    "    \"max_epoch\" : 1,\n",
    "    \"device\": 'cpu',\n",
    "    \"norm_eps\": 1e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logging(\"./Log/training.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memmap: True train data.shape: (266249, 512)\n",
      "Downloading finished...\n"
     ]
    }
   ],
   "source": [
    "dtype = 'float16'\n",
    "data_path_list=[\n",
    "    './data/pretrain_data.bin'\n",
    "]\n",
    "# Dataset Preparation\n",
    "train_ds = PretrainDataset(data_path_list, max_length=config[\"max_seq_len\"],use_memmap=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=config[\"batch_size\"],\n",
    "        pin_memory=False, drop_last=False, shuffle=False,        \n",
    "        num_workers=0 if config[\"device\"] == 'cpu' else 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(64793, 512)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-1): 2 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wk): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wv): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=512, out_features=1376, bias=False)\n",
       "        (w2): Linear(in_features=1376, out_features=512, bias=False)\n",
       "        (w3): Linear(in_features=512, out_features=1376, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=512, out_features=64793, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(config)\n",
    "model.to(config[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongpochen/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 15, with 39,498,240 parameters\n",
      "num non-decayed parameter tensors: 5, with 2,560 parameters\n",
      "using fused AdamW: False\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "optimizer = model.configure_optimizers(config[\"weight_decay\"], config[\"learning_rate\"], \n",
    "                                           (config[\"beta1\"], config[\"beta2\"]), config[\"device\"])\n",
    "raw_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size torch.Size([32, 511])\n",
      "Y size torch.Size([32, 511])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongpochen/opt/anaconda3/envs/nlp/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "2024-06-27 14:38:50,798 - INFO - step: 0, lr,  0.0000, loss:  11.1775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits size torch.Size([32, 511, 64793])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config[\"max_epoch\"]):\n",
    "        train_epoch(epoch, model, raw_model, train_loader, optimizer, scaler,\n",
    "                learning_rate = 3e-4, decay_lr = None, \n",
    "                gradient_accumulation_steps = 1, grad_clip = 1.0,\n",
    "                device = config[\"device\"])\n",
    "\n",
    "        torch.save(raw_model.state_dict(),f'Weight/epoch_{epoch}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
